import cloudscraper
from bs4 import BeautifulSoup
import json
import time

def start_scraping():
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØªØµÙØ­ ÙˆÙ‡Ù…ÙŠ Ù…ØªØ·ÙˆØ± Ù„ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ù…Ø§ÙŠØ©
    scraper = cloudscraper.create_scraper(
        browser={'browser': 'chrome','platform': 'windows','mobile': False}
    )
    
    url = "https://mangalek.com" 
    
    try:
        print("ğŸš€ Ù…Ø­Ø§ÙˆÙ„Ø© Ø³Ø­Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹...")
        # Ø¥Ø¶Ø§ÙØ© headers Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„ØªÙ…ÙˆÙŠÙ‡
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
        }
        res = scraper.get(url, headers=headers, timeout=30)
        
        if res.status_code != 200:
            print(f"âŒ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„. ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„Ø©: {res.status_code}")
            return

        soup = BeautifulSoup(res.text, "html.parser")
        manga_data = []

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¹Ù†Ø§ØµØ± (ØªØ£ÙƒØ¯Ù†Ø§ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø© Ù„Ù„Ù…ÙˆÙ‚Ø¹)
        items = soup.select('.page-item-detail, .manga-item')
        
        for index, item in enumerate(items[:20]): # Ø³Ø­Ø¨ Ø£ÙˆÙ„ 20 Ù…Ø§Ù†Ø¬Ø§
            title_el = item.select_one('h3 a')
            img_el = item.select_one('img')
            
            if title_el and img_el:
                title = title_el.get_text(strip=True)
                m_url = title_el['href']
                img = img_el.get('data-src') or img_el.get('src') or ""
                if img.startswith('//'): img = "https:" + img
                
                manga_data.append({
                    "id": index + 1000,
                    "title": title,
                    "cover": img,
                    "url": m_url,
                    "chapter": "ÙØµÙ„ Ø¬Ø¯ÙŠØ¯",
                    "rating": "4.9",
                    "age": "+13",
                    "translator": {"name": "Mohammed Elfagih", "insta": "Gremory807"}
                })

        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø­ØªÙ‰ Ù„Ùˆ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© ÙØ§Ø±ØºØ© Ù„ØªØ¬Ù†Ø¨ Ø®Ø·Ø£ Ø§Ù„Ø³ÙŠØ±ÙØ±
        if not manga_data:
            print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø§Ù†Ø¬Ø§ØŒ Ø¬Ø§Ø±ÙŠ ÙˆØ¶Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©")
            manga_data = [{"id": 1, "title": "Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ø¯ÙŠØ«...", "cover": "", "url": "#"}]

        with open('data.json', 'w', encoding='utf-8') as f:
            json.dump(manga_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù data.json Ø¨Ù†Ø¬Ø§Ø­ ÙˆØ¨Ø¯Ø§Ø®Ù„Ù‡ {len(manga_data)} Ù…Ø§Ù†Ø¬Ø§")

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ: {e}")
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ÙØ§Ø±Øº Ù„Ù…Ù†Ø¹ ØªØ¹Ø·Ù„ Ø§Ù„Ø³ÙŠØ±ÙØ±
        with open('data.json', 'w', encoding='utf-8') as f:
            json.dump([], f)

if __name__ == "__main__":
    start_scraping()
