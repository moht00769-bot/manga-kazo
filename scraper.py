import cloudscraper
from bs4 import BeautifulSoup
import json
import os

def start_scraping():
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØªØµÙØ­ ÙˆÙ‡Ù…ÙŠ Ù„ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ù…Ø§ÙŠØ©
    scraper = cloudscraper.create_scraper(browser={'browser': 'chrome','platform': 'windows','mobile': False})
    url = "https://mangalek.com" 
    
    try:
        print("ğŸ” Ù…Ø­Ø§ÙˆÙ„Ø© Ø³Ø­Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        res = scraper.get(url, timeout=30)
        soup = BeautifulSoup(res.text, "html.parser")
        manga_data = []

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø§Ù†Ø¬Ø§
        items = soup.select('.page-item-detail, .manga-item')
        
        for index, item in enumerate(items[:20]):
            title_el = item.select_one('h3 a')
            img_el = item.select_one('img')
            if title_el and img_el:
                img = img_el.get('data-src') or img_el.get('src') or ""
                if img.startswith('//'): img = "https:" + img
                manga_data.append({
                    "id": index + 1,
                    "title": title_el.get_text(strip=True),
                    "cover": img,
                    "url": title_el['href'],
                    "chapter": "ÙØµÙ„ Ø¬Ø¯ÙŠØ¯",
                    "translator": {"name": "Mohammed Elfagih", "insta": "Gremory807"}
                })

        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù„Ù…Ù†Ø¹ Ø®Ø·Ø£ Ø§Ù„Ø³ÙŠØ±ÙØ±
        if not manga_data:
            print("âš ï¸ Ù„Ù… Ù†Ø¬Ø¯ Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø³Ù†Ø¶Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¤Ù‚ØªØ©")
            manga_data = [{"id": 0, "title": "Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ø¯ÙŠØ«...", "cover": "", "url": "#"}]

        with open('data.json', 'w', encoding='utf-8') as f:
            json.dump(manga_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ data.json Ø¨Ù†Ø¬Ø§Ø­!")

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£: {e}")
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ÙØ§Ø±Øº Ø¥Ø¬Ø¨Ø§Ø±ÙŠØ§Ù‹ Ù„Ù…Ù†Ø¹ Ø§Ù„Ø³ÙŠØ±ÙØ± Ù…Ù† Ø§Ù„Ø§Ù†Ù‡ÙŠØ§Ø±
        with open('data.json', 'w', encoding='utf-8') as f:
            json.dump([], f)

if __name__ == "__main__":
    start_scraping()
